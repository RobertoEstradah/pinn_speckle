{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# ğŸ”¬ Helmholtz 2D â€” PINN con campo complejo y LHS\n\n### Proyecto: SimulaciÃ³n Acelerada de Speckle Ã“ptico\n**Roberto HernÃ¡ndez Estrada** | MaestrÃ­a en Ciencias de la ComputaciÃ³n â€” UJAT  \n**Director:** Dr. JosÃ© AdÃ¡n HernÃ¡ndez Nolasco\n\n---\n\n| Notebook | Estado |\n|---|---|\n| `01_teoria_pinns.ipynb` â€” Helmholtz 1D, validaciÃ³n del mÃ©todo | âœ… Completado â€” Error L2 = 0.002% |\n| **`02_helmholtz_2D.ipynb` â€” Helmholtz 2D, campo complejo, LHS** | ğŸ”„ En desarrollo |\n| `03_speckle.ipynb` â€” SimulaciÃ³n de speckle I = |E|Â² | ğŸ”œ Pendiente |\n| `04_benchmark.ipynb` â€” PINN vs FEniCSx (Speed-up Factor) | ğŸ”œ Pendiente |\n\n---\n\n## Â¿QuÃ© resuelve este notebook?\n\nEn el notebook 01 validaste que la PINN puede resolver Helmholtz **1D** con error L2 = 0.002%.  \nEste notebook escala al problema **real de la tesis**: Helmholtz en 2 dimensiones.\n\n$$\\nabla^2 E + k^2 E = 0 \\quad \\Longrightarrow \\quad \n\\frac{\\partial^2 E}{\\partial x^2} + \\frac{\\partial^2 E}{\\partial y^2} + (2\\pi)^2 E = 0$$\n\nLa diferencia clave: el campo Ã³ptico $E(x,y)$ es **complejo**, asÃ­ que la PINN ahora predice **dos salidas**:\n\n$$E(x,y) = E_{\\text{real}}(x,y) + i \\cdot E_{\\text{imag}}(x,y)$$\n\n## Diferencias clave respecto al notebook 01\n\n| Aspecto | Notebook 01 (1D) | Notebook 02 (2D) |\n|---|---|---|\n| Entrada de la red | $x$ (1 valor) | $(x, y)$ (2 valores) |\n| Salida de la red | $E$ real (1 valor) | $(E_{\\text{real}}, E_{\\text{imag}})$ (2 valores) |\n| EcuaciÃ³n | $E'' + k^2 E = 0$ | $\\nabla^2 E + k^2 E = 0$ |\n| Muestreo interior | Uniforme `linspace` | Latin Hypercube Sampling (LHS) |\n| Condiciones de frontera | 5 puntos conocidos | 4 bordes del dominio $[0,1]^2$ |\n| VisualizaciÃ³n | GrÃ¡fica 1D | Mapa de calor 2D (heatmap) |"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "## âš™ï¸ 0. Imports y configuraciÃ³n del entorno"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "import torch\nimport torch.nn as nn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.gridspec as gridspec\nfrom scipy.stats import qmc  # Latin Hypercube Sampling\n\n# â”€â”€ Semilla global â€” SIEMPRE al inicio para reproducibilidad â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nSEED = 42\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\nnp.random.seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark     = False\n\n# â”€â”€ Dispositivo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f'Dispositivo     : {device}')\nif torch.cuda.is_available():\n    print(f'GPU             : {torch.cuda.get_device_name(0)}')\n    print(f'VRAM total      : {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB')\nprint(f'Semilla fijada  : {SEED}')\nprint()\nprint('âœ… Entorno listo y reproducible.')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 1. Arquitectura de la red â€” PINN 2D\n\nLa red ahora recibe dos coordenadas $(x, y)$ y predice dos salidas $(E_{\\text{real}}, E_{\\text{imag}})$.  \nLa estructura interna es idÃ©ntica al notebook 01 â€” mismo nÃºmero de capas, misma activaciÃ³n `tanh`, misma inicializaciÃ³n Xavier."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ’¡ Â¿Por quÃ© dos salidas?\n\nEl campo elÃ©ctrico Ã³ptico es una cantidad **compleja**:\n\n$$E(x,y) = E_{\\text{real}}(x,y) + i \\cdot E_{\\text{imag}}(x,y)$$\n\nLas redes neuronales reales solo manejan nÃºmeros reales, asÃ­ que la estrategia estÃ¡ndar en PINNs Ã³pticas es entrenar **una sola red con dos salidas**, una por cada parte del campo.\n\nAmbas partes satisfacen la ecuaciÃ³n de Helmholtz de forma independiente:\n\n$$\\nabla^2 E_{\\text{real}} + k^2 E_{\\text{real}} = 0$$\n$$\\nabla^2 E_{\\text{imag}} + k^2 E_{\\text{imag}} = 0$$\n\nLa intensidad del speckle (lo que mide el detector) se obtiene despuÃ©s:\n\n$$I(x,y) = |E|^2 = E_{\\text{real}}^2 + E_{\\text{imag}}^2$$"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "class PINN_2D(nn.Module):\n    \"\"\"\n    Red neuronal para resolver Helmholtz 2D: âˆ‡Â²E + kÂ²E = 0\n\n    Entrada : (x, y) â€” coordenadas espaciales 2D\n    Salida  : (E_real, E_imag) â€” partes real e imaginaria del campo\n    \"\"\"\n    def __init__(self, hidden_dim=64, num_layers=5):\n        super().__init__()\n\n        # Entrada: 2 neuronas (x, y) â†’ misma arquitectura que 1D internamente\n        layers = [nn.Linear(2, hidden_dim), nn.Tanh()]\n        for _ in range(num_layers - 1):\n            layers += [nn.Linear(hidden_dim, hidden_dim), nn.Tanh()]\n        layers.append(nn.Linear(hidden_dim, 2))  # 2 salidas: E_real, E_imag\n\n        self.net = nn.Sequential(*layers)\n        self._init_weights()\n\n    def _init_weights(self):\n        \"\"\"InicializaciÃ³n Xavier â€” misma estrategia que en PINN_1D.\"\"\"\n        for layer in self.net:\n            if isinstance(layer, nn.Linear):\n                nn.init.xavier_normal_(layer.weight)\n                nn.init.zeros_(layer.bias)\n\n    def forward(self, xy):\n        \"\"\"\n        xy : tensor de forma (N, 2) con columnas [x, y]\n        Retorna tensor de forma (N, 2) con columnas [E_real, E_imag]\n        \"\"\"\n        return self.net(xy)\n\n\n# â”€â”€ Vista previa de la arquitectura â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntorch.manual_seed(SEED)\n_preview = PINN_2D(hidden_dim=64, num_layers=5)\nprint('Arquitectura PINN 2D:')\nprint(_preview)\nprint(f'\\nTotal de parÃ¡metros: {sum(p.numel() for p in _preview.parameters()):,}')\ndel _preview"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 2. Muestreo LHS â€” Latin Hypercube Sampling\n\nEn 1D usamos `linspace` para distribuir puntos uniformemente.  \nEn 2D necesitamos una estrategia mÃ¡s eficiente: **Latin Hypercube Sampling**."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ’¡ Â¿Por quÃ© LHS en lugar de una malla uniforme?\n\nCon una malla uniforme de $N \\times N$ puntos en 2D, el nÃºmero de puntos crece cuadrÃ¡ticamente.  \nPara $N=45$ ya son 2,025 puntos â€” y muchos de ellos quedan agrupados en regiones redundantes.\n\nLHS garantiza que los puntos estÃ©n **bien distribuidos en todo el dominio** con menos puntos:\n\n| Muestreo | 2000 puntos en [0,1]Â² | Cobertura | Costo |\n|---|---|---|---|\n| Malla uniforme | 44Ã—44 = 1,936 pts | Regular pero redundante | Fijo |\n| Aleatorio puro | 2000 pts | Irregular, huecos | Variable |\n| **LHS** | **2000 pts** | **Uniforme garantizada** | **Controlado** |\n\nLHS divide cada dimensiÃ³n en $N$ estratos iguales y coloca exactamente un punto en cada estrato,  \nasegurando cobertura completa del dominio sin agrupar puntos."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 3. ConfiguraciÃ³n del experimento\n\nMisma filosofÃ­a que en el notebook 01: todos los hiperparÃ¡metros en un solo lugar.  \nEl dominio es ahora $[0,1]^2$ â€” equivalente a $1\\lambda \\times 1\\lambda$ (Fase 2, validaciÃ³n 2D)."
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ’¡ SoluciÃ³n analÃ­tica en 2D\n\nPara validar la PINN necesitamos una soluciÃ³n exacta conocida.  \nUsamos una onda plana que satisface Helmholtz 2D exactamente:\n\n$$E_{\\text{real}}(x,y) = \\cos(k_x x + k_y y), \\quad E_{\\text{imag}}(x,y) = \\sin(k_x x + k_y y)$$\n\ncon $k_x^2 + k_y^2 = k^2 = (2\\pi)^2$.\n\nLa elecciÃ³n mÃ¡s simple y simÃ©trica:\n\n$$k_x = k_y = \\frac{k}{\\sqrt{2}} = \\frac{2\\pi}{\\sqrt{2}} = \\pi\\sqrt{2}$$\n\n**VerificaciÃ³n:** $k_x^2 + k_y^2 = 2\\pi^2 + 2\\pi^2 = 4\\pi^2 = k^2$ âœ…"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# â”€â”€ HiperparÃ¡metros â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nCONFIG = {\n    # â”€â”€ LÃ¡ser simulado (documentaciÃ³n â€” no entra en cÃ¡lculos de la PINN) â”€â”€â”€â”€â”€â”€\n    'lambda_laser' : 638e-9,      # diodo rojo Î» = 638 nm (computacional)\n    'L'            : 1,           # dominio = 1Ã—1 longitudes de onda (Fase 2)\n\n    # â”€â”€ FÃ­sica adimensional â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    'k'            : 2 * np.pi,   # kÌƒ adimensional â€” siempre 2Ï€\n                                   # k_x = k_y = k/sqrt(2) â†’ onda plana diagonal\n\n    # â”€â”€ Puntos de colocaciÃ³n (LHS) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    'N_colloc'     : 2000,        # puntos interiores â€” Latin Hypercube Sampling\n    'N_boundary'   : 200,         # puntos por borde (4 bordes â†’ 800 total)\n\n    # â”€â”€ Arquitectura â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    'hidden_dim'   : 64,          # neuronas por capa oculta\n    'num_layers'   : 5,           # capas ocultas\n\n    # â”€â”€ Entrenamiento â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    'n_epochs'     : 15000,       # Ã©pocas mÃ¡ximas Adam\n    'lr'           : 1e-3,        # learning rate Adam\n    'lambda_phys'  : 0.01,        # peso fÃ­sica â€” mismo valor validado en 1D\n\n    # â”€â”€ Reproducibilidad â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    'seed'         : SEED,\n\n    # â”€â”€ Meta de la tesis â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    'l2_threshold' : 0.05,        # < 5% segÃºn hipÃ³tesis del protocolo\n}\n\nprint('ConfiguraciÃ³n del experimento:')\nprint('-' * 40)\nfor key, val in CONFIG.items():\n    print(f'  {key:<16} : {val}')\nprint('-' * 40)\n\n# â”€â”€ Derivar k_x, k_y â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nk    = CONFIG['k']\nk_x  = k / np.sqrt(2)\nk_y  = k / np.sqrt(2)\nprint(f'  k_x = k/âˆš2       : {k_x:.4f}')\nprint(f'  k_y = k/âˆš2       : {k_y:.4f}')\nprint(f'  VerificaciÃ³n kÂ²  : k_xÂ²+k_yÂ² = {k_x**2 + k_y**2:.4f} = kÂ² = {k**2:.4f} âœ…')\nprint()\n\n# â”€â”€ Equivalencia fÃ­sica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nlam = CONFIG['lambda_laser']\nL   = CONFIG['L']\nprint('LÃ¡ser simulado:')\nprint(f'  Tipo                : Diodo rojo (computacional)')\nprint(f'  lambda_laser        : {lam * 1e9:.0f} nm')\nprint(f'  k adimensional      : 2Ï€ = {k:.4f}')\nprint(f'  k real equivalente  : {2 * np.pi / lam:.3e} mâ»Â¹')\nprint(f'  Dominio simulado    : {L}Ã—{L} longitudes de onda')\nprint(f'  Equivale fÃ­sicamente: {L * lam * 1e6:.4f} Ã— {L * lam * 1e6:.4f} Î¼mÂ²')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 4. Preparar datos de entrenamiento\n\n### Puntos de colocaciÃ³n â€” LHS en el interior\n### Condiciones de frontera â€” 4 bordes del dominio $[0,1]^2$"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ’¡ La funciÃ³n de pÃ©rdida PINN 2D\n\nLa pÃ©rdida total combina tres contribuciones â€” una por cada parte del campo y los datos:\n\n$$\\mathcal{L}_{\\text{total}} = \\mathcal{L}_{\\text{datos}} + \\lambda \\cdot (\\mathcal{L}_{\\text{fÃ­sica,real}} + \\mathcal{L}_{\\text{fÃ­sica,imag}})$$\n\ndonde:\n\n$$\\mathcal{L}_{\\text{datos}} = \\frac{1}{N_b} \\sum_{i=1}^{N_b} \\left[(E^{\\text{real}}_{\\text{pred},i} - E^{\\text{real}}_{\\text{BC},i})^2 + (E^{\\text{imag}}_{\\text{pred},i} - E^{\\text{imag}}_{\\text{BC},i})^2\\right]$$\n\n$$\\mathcal{L}_{\\text{fÃ­sica,real}} = \\frac{1}{N_c} \\sum_{j=1}^{N_c} \\left[\\nabla^2 E^{\\text{real}}_j + k^2 E^{\\text{real}}_j\\right]^2$$"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "k   = CONFIG['k']\nk_x = k / np.sqrt(2)\nk_y = k / np.sqrt(2)\n\n# â”€â”€ Puntos interiores con LHS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nsampler   = qmc.LatinHypercube(d=2, seed=CONFIG['seed'])\nlhs_pts   = sampler.random(n=CONFIG['N_colloc'])          # (N, 2) en [0,1]Â²\nxy_colloc = torch.tensor(lhs_pts, dtype=torch.float32).to(device)\n\nprint(f'Puntos de colocaciÃ³n (LHS): {xy_colloc.shape}  â†’ {CONFIG[\"N_colloc\"]} puntos en [0,1]Â²')\n\n# â”€â”€ Condiciones de frontera â€” 4 bordes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nN_b  = CONFIG['N_boundary']\nt    = np.linspace(0, 1, N_b)\n\n# Borde inferior: y=0, xâˆˆ[0,1]\nx_bot = np.stack([t,        np.zeros(N_b)], axis=1)\n# Borde superior: y=1, xâˆˆ[0,1]\nx_top = np.stack([t,        np.ones(N_b)],  axis=1)\n# Borde izquierdo: x=0, yâˆˆ[0,1]\nx_lft = np.stack([np.zeros(N_b), t],        axis=1)\n# Borde derecho: x=1, yâˆˆ[0,1]\nx_rgt = np.stack([np.ones(N_b),  t],        axis=1)\n\nxy_bc_np = np.vstack([x_bot, x_top, x_lft, x_rgt])   # (4*N_b, 2)\n\n# â”€â”€ SoluciÃ³n analÃ­tica en los bordes â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n# E_real(x,y) = cos(k_xÂ·x + k_yÂ·y)\n# E_imag(x,y) = sin(k_xÂ·x + k_yÂ·y)\nphase_bc      = k_x * xy_bc_np[:, 0] + k_y * xy_bc_np[:, 1]\nE_real_bc_np  = np.cos(phase_bc)\nE_imag_bc_np  = np.sin(phase_bc)\nE_bc_np       = np.stack([E_real_bc_np, E_imag_bc_np], axis=1)  # (4*N_b, 2)\n\n# â”€â”€ Convertir a tensores â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nxy_bc = torch.tensor(xy_bc_np, dtype=torch.float32).to(device)\nE_bc  = torch.tensor(E_bc_np,  dtype=torch.float32).to(device)\n\nprint(f'Puntos de frontera         : {xy_bc.shape}  â†’ {4*N_b} puntos ({N_b} por borde)')\nprint(f'  Borde inferior (y=0)     : {N_b} puntos')\nprint(f'  Borde superior (y=1)     : {N_b} puntos')\nprint(f'  Borde izquierdo (x=0)    : {N_b} puntos')\nprint(f'  Borde derecho (x=1)      : {N_b} puntos')\nprint()\nprint('Muestra de condiciones de frontera:')\nprint(f'  E(0,0)   real={E_real_bc_np[0]:.4f}   imag={E_imag_bc_np[0]:.4f}')\nprint(f'  E(0.5,0) real={np.cos(k_x*0.5):.4f}   imag={np.sin(k_x*0.5):.4f}')\nprint(f'  E(1,1)   real={np.cos(k_x+k_y):.4f}   imag={np.sin(k_x+k_y):.4f}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 5. Residuo de Helmholtz 2D y funciÃ³n de pÃ©rdida"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "def helmholtz_residual_2d(model, xy_colloc, k):\n    \"\"\"\n    Calcula el residuo de Helmholtz 2D para ambas partes del campo:\n        R_real = âˆ‚Â²E_real/âˆ‚xÂ² + âˆ‚Â²E_real/âˆ‚yÂ² + kÂ²Â·E_real\n        R_imag = âˆ‚Â²E_imag/âˆ‚xÂ² + âˆ‚Â²E_imag/âˆ‚yÂ² + kÂ²Â·E_imag\n\n    Si la red es perfecta â†’ R_real = R_imag = 0 en todo el dominio.\n    \"\"\"\n    xy = xy_colloc.clone().requires_grad_(True)\n\n    E_out  = model(xy)                        # (N, 2)\n    E_real = E_out[:, 0:1]                    # (N, 1)\n    E_imag = E_out[:, 1:2]                    # (N, 1)\n\n    ones = torch.ones_like(E_real)\n\n    def laplacian(field):\n        \"\"\"Calcula âˆ‚Â²f/âˆ‚xÂ² + âˆ‚Â²f/âˆ‚yÂ² usando autograd.\"\"\"\n        grad_f  = torch.autograd.grad(field, xy, grad_outputs=ones,\n                                      create_graph=True)[0]          # (N, 2)\n        # âˆ‚Â²/âˆ‚xÂ²\n        f_xx = torch.autograd.grad(grad_f[:, 0:1], xy, grad_outputs=ones,\n                                   create_graph=True)[0][:, 0:1]\n        # âˆ‚Â²/âˆ‚yÂ²\n        f_yy = torch.autograd.grad(grad_f[:, 1:2], xy, grad_outputs=ones,\n                                   create_graph=True)[0][:, 1:2]\n        return f_xx + f_yy\n\n    lap_real = laplacian(E_real)\n    lap_imag = laplacian(E_imag)\n\n    res_real = lap_real + k**2 * E_real\n    res_imag = lap_imag + k**2 * E_imag\n\n    return res_real, res_imag, E_real, E_imag\n\n\ndef pinn_loss_2d(model, xy_colloc, xy_bc, E_bc, k, lambda_phys):\n    \"\"\"\n    FunciÃ³n de pÃ©rdida PINN 2D:\n        L_total = L_datos + lambda * (L_fÃ­sica_real + L_fÃ­sica_imag)\n    \"\"\"\n    # â”€â”€ PÃ©rdida de datos (condiciones de frontera) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    E_pred_bc  = model(xy_bc)                            # (N_b, 2)\n    loss_data  = torch.mean((E_pred_bc - E_bc) ** 2)\n\n    # â”€â”€ PÃ©rdida fÃ­sica (residuo Helmholtz) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    res_real, res_imag, _, _ = helmholtz_residual_2d(model, xy_colloc, k)\n    loss_phys_real = torch.mean(res_real ** 2)\n    loss_phys_imag = torch.mean(res_imag ** 2)\n    loss_physics   = loss_phys_real + loss_phys_imag\n\n    # â”€â”€ Total â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n    loss_total = loss_data + lambda_phys * loss_physics\n\n    return loss_total, loss_data, loss_physics\n\n\nprint('âœ… Funciones auxiliares 2D definidas.')\nprint('   helmholtz_residual_2d() â€” calcula âˆ‡Â²E_real + kÂ²E_real  y  âˆ‡Â²E_imag + kÂ²E_imag')\nprint('   pinn_loss_2d()          â€” calcula L_total = L_datos + Î»Â·(L_real + L_imag)')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 6. Entrenamiento â€” Adam + Early Stopping + L-BFGS"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "### ğŸ’¡ Misma estrategia de entrenamiento que en 1D\n\nEl notebook 01 demostrÃ³ que la combinaciÃ³n **Adam + Early Stopping + L-BFGS** es Ã³ptima:\n\n| Fase | Optimizador | Rol |\n|---|---|---|\n| 1 | Adam | ExploraciÃ³n rÃ¡pida del espacio de parÃ¡metros |\n| â€” | Early Stopping | Detiene Adam cuando $\\mathcal{L} < 10^{-5}$ â€” evita sobreajuste |\n| 2 | L-BFGS | Ajuste fino de precisiÃ³n cerca del mÃ­nimo |\n\nLos mismos hiperparÃ¡metros validados en 1D (`lambda_phys=0.01`, `lr=1e-3`) se mantienen como punto de partida."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# â”€â”€ Semilla antes de instanciar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ntorch.manual_seed(CONFIG['seed'])\ntorch.cuda.manual_seed(CONFIG['seed'])\n\n# â”€â”€ Modelo â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel_2d = PINN_2D(\n    hidden_dim = CONFIG['hidden_dim'],\n    num_layers = CONFIG['num_layers']\n).to(device)\n\noptimizer = torch.optim.Adam(model_2d.parameters(), lr=CONFIG['lr'])\n\n# â”€â”€ Loop de entrenamiento Adam con Early Stopping â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nhistory = {'total': [], 'data': [], 'physics': []}\n\nADAM_STOP_THRESHOLD = 1e-5\n\nprint('Entrenando PINN 2D...')\nprint(f'  k={k:.4f} | LHS {CONFIG[\"N_colloc\"]} pts | {4*CONFIG[\"N_boundary\"]} pts frontera')\nprint(f'  Ã‰pocas mÃ¡x: {CONFIG[\"n_epochs\"]} | lr={CONFIG[\"lr\"]} | Î»={CONFIG[\"lambda_phys\"]}')\nprint(f'  Early stopping: L_total < {ADAM_STOP_THRESHOLD:.0e}')\nprint()\n\nepoch_final = CONFIG['n_epochs']\n\nfor epoch in range(CONFIG['n_epochs']):\n    optimizer.zero_grad()\n\n    loss, loss_data, loss_physics = pinn_loss_2d(\n        model_2d, xy_colloc, xy_bc, E_bc,\n        k           = CONFIG['k'],\n        lambda_phys = CONFIG['lambda_phys']\n    )\n\n    loss.backward()\n    optimizer.step()\n\n    history['total'].append(loss.item())\n    history['data'].append(loss_data.item())\n    history['physics'].append(loss_physics.item())\n\n    if (epoch + 1) % 500 == 0:\n        print(f'  Ã‰poca {epoch+1:5d} | '\n              f'L_total: {loss.item():.3e} | '\n              f'L_datos: {loss_data.item():.3e} | '\n              f'L_fÃ­sica: {loss_physics.item():.3e}')\n\n    if loss.item() < ADAM_STOP_THRESHOLD:\n        epoch_final = epoch + 1\n        print(f'\\n  â¹ Early stopping en Ã©poca {epoch_final} â€” '\n              f'L_total={loss.item():.3e} < {ADAM_STOP_THRESHOLD:.0e}')\n        print(f'  Ã‰pocas ahorradas: {CONFIG[\"n_epochs\"] - epoch_final}')\n        break\n\nprint(f'\\nâœ… Adam completado en {epoch_final} Ã©pocas.')\n\n# â”€â”€ Ajuste fino con L-BFGS â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint('\\nAjuste fino con L-BFGS...')\n\noptimizer_lbfgs = torch.optim.LBFGS(\n    model_2d.parameters(),\n    lr             = 1.0,\n    max_iter       = 500,\n    history_size   = 50,\n    line_search_fn = 'strong_wolfe'\n)\n\nhistory_lbfgs = {'total': [], 'data': [], 'physics': []}\niter_count = [0]\n\ndef closure():\n    optimizer_lbfgs.zero_grad()\n    loss, loss_data, loss_physics = pinn_loss_2d(\n        model_2d, xy_colloc, xy_bc, E_bc,\n        k           = CONFIG['k'],\n        lambda_phys = CONFIG['lambda_phys']\n    )\n    loss.backward()\n    history_lbfgs['total'].append(loss.item())\n    history_lbfgs['data'].append(loss_data.item())\n    history_lbfgs['physics'].append(loss_physics.item())\n    iter_count[0] += 1\n    if iter_count[0] % 100 == 0:\n        print(f'  Iter {iter_count[0]:4d} | '\n              f'L_total: {loss.item():.3e} | '\n              f'L_datos: {loss_data.item():.3e} | '\n              f'L_fÃ­sica: {loss_physics.item():.3e}')\n    return loss\n\noptimizer_lbfgs.step(closure)\n\nprint('\\nâœ… L-BFGS completado.')\nprint(f'   Ã‰pocas Adam     : {epoch_final} / {CONFIG[\"n_epochs\"]}')\nprint(f'   Ã‰pocas ahorradas: {CONFIG[\"n_epochs\"] - epoch_final}')\nprint(f'   Iters L-BFGS    : {iter_count[0]}')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ“Œ 7. EvaluaciÃ³n y visualizaciÃ³n de resultados\n\nEvaluamos la PINN en una malla uniforme de $100 \\times 100$ puntos (10,000 puntos de prueba)  \ny comparamos contra la soluciÃ³n analÃ­tica exacta."
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# â”€â”€ Malla de evaluaciÃ³n 100Ã—100 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nN_eval = 100\nx_lin  = np.linspace(0, 1, N_eval)\ny_lin  = np.linspace(0, 1, N_eval)\nXX, YY = np.meshgrid(x_lin, y_lin)\n\nxy_test_np = np.stack([XX.ravel(), YY.ravel()], axis=1)  # (10000, 2)\nxy_test    = torch.tensor(xy_test_np, dtype=torch.float32).to(device)\n\n# â”€â”€ PredicciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nmodel_2d.eval()\nwith torch.no_grad():\n    E_pred = model_2d(xy_test).cpu().numpy()   # (10000, 2)\n\nE_real_pred = E_pred[:, 0].reshape(N_eval, N_eval)\nE_imag_pred = E_pred[:, 1].reshape(N_eval, N_eval)\n\n# â”€â”€ SoluciÃ³n analÃ­tica â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nphase_test   = k_x * XX + k_y * YY\nE_real_exact = np.cos(phase_test)\nE_imag_exact = np.sin(phase_test)\nI_exact      = E_real_exact**2 + E_imag_exact**2  # = 1 para onda plana\n\n# â”€â”€ Intensidad PINN â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nI_pred = E_real_pred**2 + E_imag_pred**2\n\n# â”€â”€ MÃ©tricas â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\ndef l2_rel(pred, exact):\n    return np.linalg.norm(pred.ravel() - exact.ravel()) / np.linalg.norm(exact.ravel())\n\nl2_real = l2_rel(E_real_pred, E_real_exact)\nl2_imag = l2_rel(E_imag_pred, E_imag_exact)\nl2_mean = (l2_real + l2_imag) / 2\ncumple  = l2_mean < CONFIG['l2_threshold']\n\nerror_real = np.abs(E_real_pred - E_real_exact)\nerror_imag = np.abs(E_imag_pred - E_imag_exact)\n\nprint(f'MÃ©tricas de evaluaciÃ³n (malla {N_eval}Ã—{N_eval}):')\nprint(f'  Error L2 E_real   : {l2_real*100:.3f}%')\nprint(f'  Error L2 E_imag   : {l2_imag*100:.3f}%')\nprint(f'  Error L2 promedio : {l2_mean*100:.3f}%')\nprint(f'  Umbral tesis      : < {CONFIG[\"l2_threshold\"]*100:.0f}%')\nprint(f'  Estado            : {\"âœ… CUMPLE\" if cumple else \"âš  AÃšN NO CUMPLE\"}'  )"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# â”€â”€ VisualizaciÃ³n â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nfig = plt.figure(figsize=(18, 14))\ngs  = gridspec.GridSpec(3, 3, figure=fig, hspace=0.45, wspace=0.35)\n\ncmap_field = 'RdBu_r'\ncmap_error = 'hot_r'\n\n# â”€â”€ Fila 1: E_real â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nax = fig.add_subplot(gs[0, 0])\nim = ax.imshow(E_real_exact, origin='lower', extent=[0,1,0,1],\n               cmap=cmap_field, vmin=-1, vmax=1)\nax.set_title('E_real â€” SoluciÃ³n exacta', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\nax = fig.add_subplot(gs[0, 1])\nim = ax.imshow(E_real_pred, origin='lower', extent=[0,1,0,1],\n               cmap=cmap_field, vmin=-1, vmax=1)\nax.set_title('E_real â€” PredicciÃ³n PINN', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\nax = fig.add_subplot(gs[0, 2])\nim = ax.imshow(error_real, origin='lower', extent=[0,1,0,1],\n               cmap=cmap_error)\nax.set_title(f'|Error| E_real â€” L2 = {l2_real*100:.3f}%', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\n# â”€â”€ Fila 2: E_imag â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nax = fig.add_subplot(gs[1, 0])\nim = ax.imshow(E_imag_exact, origin='lower', extent=[0,1,0,1],\n               cmap=cmap_field, vmin=-1, vmax=1)\nax.set_title('E_imag â€” SoluciÃ³n exacta', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\nax = fig.add_subplot(gs[1, 1])\nim = ax.imshow(E_imag_pred, origin='lower', extent=[0,1,0,1],\n               cmap=cmap_field, vmin=-1, vmax=1)\nax.set_title('E_imag â€” PredicciÃ³n PINN', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\nax = fig.add_subplot(gs[1, 2])\nim = ax.imshow(error_imag, origin='lower', extent=[0,1,0,1],\n               cmap=cmap_error)\nax.set_title(f'|Error| E_imag â€” L2 = {l2_imag*100:.3f}%', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\n# â”€â”€ Fila 3: Intensidad y pÃ©rdida â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nax = fig.add_subplot(gs[2, 0])\nim = ax.imshow(I_pred, origin='lower', extent=[0,1,0,1], cmap='inferno')\nax.set_title('Intensidad PINN: I = |E|Â²', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nplt.colorbar(im, ax=ax)\n\n# DistribuciÃ³n LHS\nax = fig.add_subplot(gs[2, 1])\nxy_lhs_np = xy_colloc.cpu().numpy()\nax.scatter(xy_lhs_np[:, 0], xy_lhs_np[:, 1], s=1, alpha=0.4, c='steelblue')\nax.set_title(f'Puntos LHS ({CONFIG[\"N_colloc\"]} pts)', fontsize=11, fontweight='bold')\nax.set_xlabel('x'); ax.set_ylabel('y')\nax.set_xlim(0, 1); ax.set_ylim(0, 1)\nax.set_aspect('equal')\n\n# Curvas de pÃ©rdida\nax = fig.add_subplot(gs[2, 2])\nep_adam  = range(1, epoch_final + 1)\nep_lbfgs = range(epoch_final, epoch_final + len(history_lbfgs['total']))\nax.semilogy(ep_adam, history['total'][:epoch_final],   'k-',  lw=2,   label='L_total (Adam)')\nax.semilogy(ep_adam, history['data'][:epoch_final],    'b--', lw=1.5, label='L_datos (Adam)')\nax.semilogy(ep_adam, history['physics'][:epoch_final], 'r--', lw=1.5, label='L_fÃ­sica (Adam)')\nax.semilogy(ep_lbfgs, history_lbfgs['total'],   'k-',  lw=2,   alpha=0.4, label='L_total (L-BFGS)')\nax.semilogy(ep_lbfgs, history_lbfgs['data'],    'b--', lw=1.5, alpha=0.4, label='L_datos (L-BFGS)')\nax.semilogy(ep_lbfgs, history_lbfgs['physics'], 'r--', lw=1.5, alpha=0.4, label='L_fÃ­sica (L-BFGS)')\nax.axvline(x=epoch_final, color='gray', ls=':', lw=2, label=f'Early stop Ã©.{epoch_final}')\nax.set_title('Curvas de pÃ©rdida', fontsize=11, fontweight='bold')\nax.set_xlabel('Ã‰poca / IteraciÃ³n'); ax.set_ylabel('Loss (log)')\nax.legend(fontsize=8); ax.grid(True, alpha=0.3)\n\nplt.suptitle(\n    f'PINN 2D â€” Helmholtz | k=2Ï€ | Error L2 promedio = {l2_mean*100:.3f}% | '\n    f'Meta < {CONFIG[\"l2_threshold\"]*100:.0f}%',\n    fontsize=14, fontweight='bold'\n)\nplt.savefig('resultados_pinn_2d.png', dpi=150, bbox_inches='tight')\nplt.show()"
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": "# â”€â”€ Resumen final â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\nprint('=' * 55)\nprint('RESUMEN DE RESULTADOS â€” PINN 2D')\nprint('=' * 55)\nprint(f'  Semilla               : {CONFIG[\"seed\"]}')\nprint(f'  k adimensional        : 2Ï€ = {k:.4f}')\nprint(f'  k_x = k_y             : {k_x:.4f}')\nprint()\nprint(f'  Error L2 E_real       : {l2_real:.6f}  ({l2_real*100:.3f}%)')\nprint(f'  Error L2 E_imag       : {l2_imag:.6f}  ({l2_imag*100:.3f}%)')\nprint(f'  Error L2 promedio     : {l2_mean:.6f}  ({l2_mean*100:.3f}%)')\nprint(f'  Error mÃ¡x E_real      : {error_real.max():.6f}')\nprint(f'  Error mÃ¡x E_imag      : {error_imag.max():.6f}')\nprint(f'  Umbral tesis (L2)     : < {CONFIG[\"l2_threshold\"]*100:.0f}%')\nprint(f'  Estado                : {\"âœ… CUMPLE\" if cumple else \"âš  AÃšN NO CUMPLE â€” ajustar hiperparÃ¡metros\"}')\nprint()\nprint('Entrenamiento:')\nprint(f'  Ã‰pocas Adam           : {epoch_final} / {CONFIG[\"n_epochs\"]}')\nprint(f'  Ã‰pocas ahorradas      : {CONFIG[\"n_epochs\"] - epoch_final}')\nprint(f'  Iteraciones L-BFGS    : {iter_count[0]}')\nprint()\nprint('HiperparÃ¡metros:')\nprint(f'  hidden_dim            = {CONFIG[\"hidden_dim\"]}')\nprint(f'  num_layers            = {CONFIG[\"num_layers\"]}')\nprint(f'  N_colloc (LHS)        = {CONFIG[\"N_colloc\"]}')\nprint(f'  N_boundary (por borde)= {CONFIG[\"N_boundary\"]}  â†’ {4*CONFIG[\"N_boundary\"]} total')\nprint(f'  lambda_phys           = {CONFIG[\"lambda_phys\"]}')\nprint()\nlam = CONFIG['lambda_laser']\nL   = CONFIG['L']\nprint('LÃ¡ser simulado:')\nprint(f'  Tipo                  : Diodo rojo (computacional)')\nprint(f'  lambda_laser          : {lam * 1e9:.0f} nm')\nprint(f'  k adimensional        : 2Ï€ = {k:.4f}')\nprint(f'  k real equivalente    : {2 * np.pi / lam:.3e} mâ»Â¹')\nprint(f'  Dominio simulado      : {L}Ã—{L} longitudes de onda')\nprint(f'  Equivale fÃ­sicamente  : {L * lam * 1e6:.4f} Ã— {L * lam * 1e6:.4f} Î¼mÂ²')"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## ğŸ—ºï¸ Â¿QuÃ© sigue?\n\nCon este notebook validaste que la PINN resuelve Helmholtz **2D** con campo complejo.  \nEl siguiente paso genera el **patrÃ³n de speckle Ã³ptico** â€” el objetivo central de la tesis:\n\n| Notebook | Contenido |\n|---|---|\n| `01_teoria_pinns.ipynb` âœ… | Helmholtz 1D â€” validaciÃ³n del mÃ©todo |\n| **`02_helmholtz_2D.ipynb` âœ…** | **Helmholtz 2D â€” campo complejo, LHS** |\n| `03_speckle.ipynb` ğŸ”œ | $I(x,y) = |E|^2$ â€” patrÃ³n de speckle + estadÃ­stica |\n| `04_benchmark.ipynb` ğŸ”œ | PINN vs FEniCSx â€” Speed-up Factor |\n\n## ğŸ“‹ Lecciones aprendidas en 2D\n\n| Aspecto | DecisiÃ³n |\n|---|---|\n| Campo complejo | Una red, dos salidas: $(E_{\\text{real}}, E_{\\text{imag}})$ |\n| Muestreo interior | LHS â€” cobertura uniforme garantizada |\n| Condiciones de frontera | 4 bordes completos, $N_b$ puntos por borde |\n| Residuo Helmholtz | Laplaciano vÃ­a autograd en dos dimensiones |\n| PÃ©rdida | $\\mathcal{L} = \\mathcal{L}_{\\text{datos}} + \\lambda(\\mathcal{L}_{\\text{real}} + \\mathcal{L}_{\\text{imag}})$ |\n| Estrategia | Adam + Early Stopping + L-BFGS â€” misma que 1D |\n\n> **Nota:** El notebook 03 tomarÃ¡ la predicciÃ³n $E(x,y)$ de este notebook  \n> y calcularÃ¡ $I = |E|^2$ para generar y analizar el patrÃ³n de speckle."
  }
 ]
}